---
customer: "TELUS HEALTH"
title: "Strategic Primers"
date: "2025-10-20"
task: "Lead UX Researcher"
company: "Developed for TELUS Health"
activities: "Category"
when: "2025"
---

## 1) Executive Summary (TL;DR)

Turning a year of scattered signals into roadmap-ready priorities

I created an executive-ready set of strategic UX priorities (with clear evidence and recommendations) to influence planning conversations and reduce “known-known” UX debt becoming normalized (IA, support experience, technical reliability, content relevance).

**My role** Lead UX Researcher and strategic partner. Owned synthesis across sources, built the narrative, and delivered the cross-functional “planning input” (primers + deck/script).

**Team / collaborators**  

Cross-functional partnership with Product and Design leadership as the primary audience for roadmap influence. (Specific names/roles are not captured in the pages I can see, aside from a presentation handoff/thanks to “Andy.”)

## 2) The Challenge & Context

**The problem space (the “why now”)**  

Telus Health One had accumulated a large volume of user signals across time and channels, but decision-making risked being driven by the loudest anecdote or the newest request. The business needed a concise, credible, planning-friendly synthesis that made it easy for PMs to prioritize the highest-impact experience issues ahead of roadmap conversations.

**Research goals (knowledge gaps to bridge)**  

- What are the *few* experience themes that consistently drive negative outcomes (friction, abandonment, dissatisfaction) across the ecosystem of signals?
- How do we quantify and triangulate those themes enough that they become “roadmap inputs,” not just research findings?
- Where do competitors set user expectations that raise the bar for TH1?

**Constraints (what this shows about seniority)**

- **Data heterogeneity:** Many sources, different levels of rigor and recency, plus known biases (e.g., app-store review extremity).
- **Planning reality:** Output needed to be digestible and actionable for time-poor stakeholders (“less than a minute” scan, then deeper read).
- **Privacy and representativeness limits:** Quotes were sometimes paraphrased/aggregated to protect identities; competitor comparisons are inherently imperfect.

## 3) Strategic Approach & Methodology (and why these choices)

**Method selection (why this mix)**

This was a *strategic synthesis* effort, not a single study. The approach deliberately used triangulation to increase credibility and reduce single-source bias:

- **Competitor app review analysis** to understand expectation-setting and positioning.
- **Behavioral analytics patterns** (engagement, path complexity, retention drop-off) to connect complaints to observable product behavior.
- **Ongoing / collected user feedback** to capture breadth of pain points and language users actually use.
- **UXPI / UX measures** to anchor discussions in benchmarkable metrics (even if imperfect).

**Participant / sample strategy (how you ensured “the right people”)**  

Instead of recruiting a fresh sample, the strategy was to **aggregate signals from the real user population** already speaking through multiple channels (reviews, continuous measures, feedback streams), then validate themes via convergence across sources.

*Lead-level note:* this is a pragmatic approach when the business needs directional clarity fast, and when additional recruitment would not change the top-of-funnel strategic questions.

**Scaling / leverage (multiplying research value)**  

The “strategic primer” format itself is a scaling mechanism: it turns research into a reusable planning asset and a shared language for prioritization across teams.

## 4) The “Messy Middle” (Process & Collaboration)

**Stakeholder alignment (how buy-in was earned)**

- Positioned the work as a **planning input** with a clear narrative arc: “why primers,” “what data,” “what priorities,” “so what for roadmap.”
- Framed findings in terms stakeholders act on: business levers like retention, cost reduction, and brand trust, rather than research-only language.
**The pivot (navigating ambiguity)**

A key leadership move here is acknowledging that, with “a year’s worth” of inputs, the hard part is *reducing scope without losing truth*. The script explicitly calls out the tension between “primers should be discrete” vs. the reality of the dataset, and shows judgment in making it “as small as possible.”

**Synthesis (showing just enough of the engine)**

- Used **triangulation** explicitly as the synthesis strategy to improve validity.
- Carried limitations forward (sampling bias, paraphrasing/aggregation, platform skew) to prevent overclaiming and build credibility with senior stakeholders.

## 5) Insights & Actionable Recommendations (3–4 “Aha” moments)

### Finding 1: Information architecture is a structural blocker, not a usability “nit”

**What’s happening**: Users struggle to find categories, get sent in circles, and fall into “rabbit hole” navigation patterns. 

**Recommendation bridge**: Redesign navigation/IA with clearer labeling, progressive disclosure, and improved information scent so people can reliably reach services and content.

### Finding 2: Support needs to feel human, especially in high-stress moments

**What’s happening**: Users describe support as slow, transactional, and overly gatekept when they are seeking help.

**Recommendation bridge**: Standardize support protocols and design for “warm handoffs,” set expectations, and benchmark empathetic competitors.

### Finding 3: Technical reliability is the table-stakes trust layer

**What’s happening**: Crashes, login failures, and “technical issues” messages show up at the worst possible time, undermining trust and driving abandonment.
**Recommendation bridge**: Prioritize infrastructure, error handling, and monitoring as a top roadmap item, not a backlog cleanup.

### Finding 4: Content relevance drives whether users stay in-product or flee to Google

**What’s happening**: Users report content as generic and not tailored; they resort to search, then external search, which is a signal the product isn’t meeting intent.

**Recommendation bridge**: Improve personalization and content pathways, and create feedback loops to tune relevance over time.

## 6) Business Impact & Outcomes (what you can credibly claim *from this folder*)

**Product influence (direct, portfolio-safe claims)**

- Produced a **roadmap-ready prioritization frame**: IA, support experience, technical reliability, and content relevance as the core strategic areas.
- Created an **exec narrative + presentation script** designed to land in planning conversations and travel across teams (not just sit in a research repo).

**Measurable impact (what’s missing, and how to write it honestly)**

This folder includes baseline-like metrics and comparative signals (e.g., sentiment benchmarks and UXPI framing), but it does *not* include post-launch outcomes (conversion, churn, reduced tickets, etc.) in the pages I can see.

So, for a portfolio, you can write this section in one of these “truthful” ways:

- **If outcomes exist but are confidential:**
    
    “Influenced the H2 roadmap by securing agreement to prioritize reliability and IA workstreams; outcome metrics tracked internally (retention, support contact rate, and task success), but not shareable publicly.”
    
- **If outcomes were not tracked:**
    
    “Key learning: strategic synthesis only becomes ‘impact’ when paired with agreed success metrics. In hindsight, I would have defined a measurement plan upfront (e.g., login success rate, week-1 retention, support time-to-resolution, and search-to-content engagement).”
    
**Reflections (research maturity signal)**

This project demonstrates a research maturity shift: moving from individual study readouts to **institutionalized synthesis** that supports planning cycles and aligns teams on “known-knowns” with evidence and constraints stated clearly.

### 3 quick questions to finalize this into a “ready to ship” portfolio piece

1) What was the *actual moment of influence* (roadmap meeting, Q-planning, leadership review) and who was in the room? (Roles are enough.)  

2) Did any of the four priority areas lead to a shipped change? If yes, what metric moved (even directional)?  

3) Is there a safe way to reference the PDF report findings (e.g., 1–2 non-sensitive charts/figures), or should we keep it fully text-only?

If you want me to incorporate the PDF details directly, I’ll need the PDF content to be viewable in full (right now it’s truncated in the attachment preview).

<figure class="figure">
	<img src="./perks-data.png">
    <figcaption>data</figcaption>
</figure>

